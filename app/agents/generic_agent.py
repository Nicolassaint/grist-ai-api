import openai
from typing import Dict, Any, Optional
from ..models.message import Message, ConversationHistory
from ..utils.logging import AgentLogger
from ..utils.conversation_formatter import (
    format_conversation_for_llm_messages,
    should_include_conversation_history,
)
import time


class GenericAgent:
    """Agent principal pour les questions g√©n√©rales et le petit talk"""

    def __init__(self, openai_client: openai.AsyncOpenAI, model: str = "gpt-3.5-turbo"):
        self.client = openai_client
        self.model = model
        self.logger = AgentLogger("generic_agent")

        self.system_prompt = """Tu es un assistant IA int√©gr√© √† Grist, une plateforme de gestion de donn√©es.

Ton r√¥le est de :
- R√©pondre aux questions g√©n√©rales sur Grist et ses fonctionnalit√©s
- Aider l'utilisateur √† comprendre comment utiliser le widget IA
- Faire du petit talk de mani√®re amicale et professionnelle
- Guider l'utilisateur vers les bonnes questions pour analyser ses donn√©es

Contexte : L'utilisateur travaille avec un document Grist contenant des donn√©es qu'il peut analyser via ce widget IA.

Instructions :
- Sois amical, utile et professionnel
- Reste concis dans tes r√©ponses (max 150 mots)
- Si l'utilisateur pose une question sur des donn√©es sp√©cifiques, sugg√®re-lui de reformuler pour une analyse de donn√©es
- N'invente jamais de donn√©es ou d'informations sp√©cifiques au document de l'utilisateur

Exemples de r√©ponses appropri√©es :
- Salutations et pr√©sentations
- Explication des capacit√©s du widget
- Conseils sur comment poser des questions d'analyse
- Aide g√©n√©rale sur Grist"""

    async def process_message(
        self,
        user_message: str,
        conversation_history: ConversationHistory,
        request_id: str,
    ) -> str:
        """Traite un message g√©n√©rique"""
        start_time = time.time()

        self.logger.log_agent_start("generic", user_message[:80])

        try:
            # Construction du contexte conversationnel
            messages = [{"role": "system", "content": self.system_prompt}]

            # Ajout de l'historique de conversation format√© (paires user/assistant compl√®tes)
            if should_include_conversation_history("generic"):
                history_messages = format_conversation_for_llm_messages(
                    conversation_history, max_pairs=3
                )
                messages.extend(history_messages)

            # ü§ñ Log lisible de la requ√™te IA
            prompt_text = "\n".join(
                [f"{msg['role']}: {msg['content']}" for msg in messages]
            )
            self.logger.log_ai_request(
                model=self.model,
                messages_count=len(messages),
                max_tokens=200,
                request_id=request_id,
                prompt_preview=prompt_text,
            )

            response = await self.client.chat.completions.create(
                model=self.model, messages=messages, max_tokens=200, temperature=0.7
            )

            ai_response = response.choices[0].message.content.strip()

            # ü§ñ Log lisible de la r√©ponse IA
            tokens_used = (
                getattr(response.usage, "total_tokens", None)
                if hasattr(response, "usage")
                else None
            )
            self.logger.log_ai_response(
                model=self.model,
                tokens_used=tokens_used,
                success=True,
                request_id=request_id,
                response_preview=ai_response,
            )

            execution_time = time.time() - start_time
            self.logger.log_agent_response("generic", True, execution_time)

            return ai_response

        except Exception as e:
            execution_time = time.time() - start_time

            # ü§ñ Log lisible d'erreur IA
            self.logger.log_ai_response(
                model=self.model, success=False, request_id=request_id
            )

            self.logger.error(
                f"Erreur lors du traitement g√©n√©rique: {str(e)}",
                request_id=request_id,
                execution_time=execution_time,
            )
            return self._get_fallback_response(user_message)

    def _get_fallback_response(self, user_message: str) -> str:
        """R√©ponse de secours en cas d'erreur"""
        user_lower = user_message.lower()

        if any(
            greeting in user_lower for greeting in ["bonjour", "salut", "hello", "hey"]
        ):
            return "Bonjour ! Je suis votre assistant IA pour Grist. Comment puis-je vous aider aujourd'hui ?"

        elif any(help_word in user_lower for help_word in ["aide", "help", "comment"]):
            return (
                "Je peux vous aider √† analyser vos donn√©es Grist ! "
                "Posez-moi des questions sur vos donn√©es ou demandez-moi de g√©n√©rer des analyses. "
                "Par exemple : 'Montre-moi les tendances de ventes' ou 'Combien d'utilisateurs avons-nous ?'"
            )

        elif any(what_word in user_lower for what_word in ["quoi", "what", "que"]):
            return (
                "Je suis un assistant IA int√©gr√© √† votre document Grist. "
                "Je peux analyser vos donn√©es, g√©n√©rer des requ√™tes SQL, et r√©pondre √† vos questions g√©n√©rales. "
                "Que souhaitez-vous savoir sur vos donn√©es ?"
            )

        else:
            return (
                "D√©sol√©, je rencontre une difficult√© technique temporaire. "
                "Pouvez-vous reformuler votre question ? Je suis l√† pour vous aider avec vos donn√©es Grist !"
            )

    def _detect_data_question(self, message: str) -> bool:
        """D√©tecte si la question concerne des donn√©es sp√©cifiques"""
        data_indicators = [
            "donn√©es",
            "table",
            "colonne",
            "ligne",
            "enregistrement",
            "vente",
            "client",
            "utilisateur",
            "commande",
            "produit",
            "analyse",
            "statistique",
            "tendance",
            "total",
            "moyenne",
            "maximum",
            "minimum",
            "count",
            "sum",
        ]

        message_lower = message.lower()
        return any(indicator in message_lower for indicator in data_indicators)

    def suggest_data_analysis(self, user_message: str) -> str:
        """Sugg√®re comment reformuler pour une analyse de donn√©es"""
        suggestions = [
            "Pour analyser vos donn√©es, essayez des questions comme :",
            "‚Ä¢ 'Montre-moi les ventes du mois dernier'",
            "‚Ä¢ 'Combien d'utilisateurs actifs avons-nous ?'",
            "‚Ä¢ 'Analyse les tendances de nos produits'",
            "‚Ä¢ 'Quelle est la moyenne des commandes par client ?'",
            "",
            "Je peux acc√©der √† vos donn√©es Grist et g√©n√©rer des analyses d√©taill√©es !",
        ]

        return "\n".join(suggestions)
