import httpx
from typing import Dict, List, Any, Optional
from ..utils.logging import AgentLogger


class GristContentFetcher:
    """R√©cup√®re les noms des tables et leur contenu depuis l'API Grist"""
    
    def __init__(self, api_key: str, base_url: str = "https://docs.getgrist.com/api"):
        self.api_key = api_key
        self.base_url = base_url.rstrip('/')
        self.logger = AgentLogger("grist_content_fetcher")
        
        # Headers par d√©faut
        self.headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }
    
    async def get_document_tables(self, document_id: str, request_id: str = "unknown") -> List[str]:
        """R√©cup√®re la liste des tables d'un document"""
        url = f"{self.base_url}/docs/{document_id}/tables"
        
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(url, headers=self.headers)
                self.logger.log_grist_api(url, response.status_code)
                
                if response.status_code == 200:
                    data = response.json()
                    tables = [table["id"] for table in data.get("tables", [])]
                    
                    # Logs d√©taill√©s des donn√©es re√ßues
                    self.logger.info(
                        "üìã Tables r√©cup√©r√©es depuis Grist",
                        request_id=request_id,
                        document_id=document_id,
                        tables_count=len(tables),
                        tables_list=tables,
                        raw_data_size=len(str(data))
                    )
                    
                    self.logger.info(f"Tables r√©cup√©r√©es: {tables}", request_id=request_id)
                    return tables
                else:
                    self.logger.error(
                        f"Erreur lors de la r√©cup√©ration des tables: {response.status_code}",
                        request_id=request_id,
                        response_text=response.text
                    )
                    return []
                    
        except Exception as e:
            self.logger.error(f"Exception lors de la r√©cup√©ration des tables: {str(e)}", request_id=request_id)
            return []
    
    async def get_table_content_csv(self, document_id: str, table_id: str, request_id: str = "unknown") -> str:
        """R√©cup√®re le contenu d'une table sp√©cifique en format CSV sous forme de string"""
        url = f"{self.base_url}/docs/{document_id}/download/csv"
        
        # Param√®tres pour sp√©cifier la table et le format des headers
        params = {
            "tableId": table_id,
            "header": "label"  # Utilise les labels des colonnes plut√¥t que les colIds
        }
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=self.headers, params=params)
                self.logger.log_grist_api(url, response.status_code)
                
                if response.status_code == 200:
                    # Retourner directement le contenu CSV comme string
                    csv_content = response.text
                    
                    # Logs simplifi√©s sans parsing
                    lines = csv_content.strip().split('\n')
                    line_count = len(lines)
                    
                    self.logger.info(
                        "üìä Contenu CSV r√©cup√©r√© depuis Grist",
                        request_id=request_id,
                        table_id=table_id,
                        line_count=line_count,
                        content_size=len(csv_content)
                    )
                    
                    if line_count > 1:
                        self.logger.info(
                            "üìã Aper√ßu du contenu de la table",
                            request_id=request_id,
                            table_id=table_id,
                            first_lines=lines[:3] if len(lines) > 3 else lines
                        )
                    else:
                        self.logger.info(
                            "‚úÖ Table r√©cup√©r√©e avec succ√®s mais vide",
                            request_id=request_id,
                            table_id=table_id,
                            note="La table ne contient aucune donn√©e - c'est normal pour une table vide"
                        )
                    
                    self.logger.info(
                        f"Contenu de table r√©cup√©r√©: {table_id}",
                        request_id=request_id,
                        row_count=len(rows),
                        columns_count=len(columns)
                    )
                    
                    return csv_content
                    
                else:
                    error_msg = f"Erreur HTTP {response.status_code}: {response.text}"
                    self.logger.error(
                        "Erreur lors de la r√©cup√©ration du contenu CSV",
                        request_id=request_id,
                        table_id=table_id,
                        status_code=response.status_code,
                        response_text=response.text
                    )
                    return f"# Erreur lors de la r√©cup√©ration de la table {table_id}\n# {error_msg}"
                    
        except httpx.TimeoutException:
            error_msg = "Timeout lors de la r√©cup√©ration du contenu CSV"
            self.logger.error(error_msg, request_id=request_id, table_id=table_id)
            return f"# Erreur Timeout pour la table {table_id}\n# {error_msg}"
            
        except Exception as e:
            error_msg = f"Exception lors de la r√©cup√©ration du contenu CSV: {str(e)}"
            self.logger.error(error_msg, request_id=request_id, table_id=table_id)
            return f"# Erreur Exception pour la table {table_id}\n# {error_msg}"
    
    async def get_all_tables_content(self, document_id: str, request_id: str = "unknown") -> str:
        """R√©cup√®re le contenu de toutes les tables d'un document en format CSV concat√©n√©"""
        tables = await self.get_document_tables(document_id, request_id)
        
        if not tables:
            self.logger.warning("Aucune table trouv√©e dans le document", request_id=request_id)
            return "# Aucune table trouv√©e dans le document"
        
        all_csv_content = []
        total_rows = 0
        
        for table_id in tables:
            csv_content = await self.get_table_content_csv(document_id, table_id, request_id)
            
            # Ajouter un header pour identifier la table
            table_section = f"\n# ===== TABLE: {table_id} =====\n"
            table_section += csv_content
            table_section += f"\n# ===== FIN TABLE: {table_id} =====\n"
            
            all_csv_content.append(table_section)
            
            # Compter les lignes pour les logs (en excluant l'header)
            if not csv_content.startswith("#"):  # Si ce n'est pas une erreur
                lines = csv_content.strip().split('\n')
                if len(lines) > 1:  # Header + donn√©es
                    total_rows += len(lines) - 1
        
        # Assembler tout le contenu
        final_content = f"# Document Grist ID: {document_id}\n"
        final_content += f"# Nombre de tables: {len(tables)}\n"
        final_content += f"# Tables: {', '.join(tables)}\n"
        final_content += "".join(all_csv_content)
        
        # Log d√©taill√© des contenus finaux
        self.logger.info(
            "üìö Tous les contenus de tables assembl√©s en CSV",
            request_id=request_id,
            document_id=document_id,
            tables_count=len(tables),
            total_rows=total_rows,
            total_csv_size=len(final_content),
            tables_list=tables
        )
        
        self.logger.info(
            f"Tous les contenus de tables r√©cup√©r√©s en format CSV",
            request_id=request_id,
            document_id=document_id,
            tables_count=len(tables),
            total_rows=total_rows
        )
        return final_content

        
    async def get_all_tables_preview(self, document_id: str, request_id: str = "unknown") -> str:
        """R√©cup√®re un aper√ßu de toutes les tables d'un document via l'endpoint /records"""
        try:
            # R√©cup√©ration des tables du document
            tables = await self.get_document_tables(document_id, request_id)
            
            if not tables:
                self.logger.warning("Aucune table trouv√©e dans le document", request_id=request_id)
                return "# Aucune table trouv√©e dans le document"
            
            preview_parts = []
            
            # R√©cup√©rer un aper√ßu de TOUTES les tables via /records
            for table_id in tables:
                try:
                    # Utilisation de format_csv_content_for_analysis qui utilise /records
                    formatted_content = await self.format_csv_content_for_analysis(
                        document_id, table_id, request_id, limit=10
                    )
                    
                    if formatted_content and not formatted_content.startswith("Erreur"):
                        preview_parts.append(formatted_content)
                    else:
                        # En cas d'erreur, ajouter le message d'erreur
                        preview_parts.append(f"# Erreur lors de la r√©cup√©ration de la table {table_id}: {formatted_content}")
                        
                except Exception as e:
                    self.logger.warning(f"Erreur lors de la r√©cup√©ration de l'aper√ßu de {table_id}: {str(e)}")
                    preview_parts.append(f"# Erreur lors de la r√©cup√©ration de la table {table_id}: {str(e)}")
                    continue
            
            # Assemblage final
            if preview_parts:
                final_content = f"# Aper√ßu du document Grist ID: {document_id}\n"
                final_content += f"# Nombre de tables: {len(tables)}\n"
                final_content += f"# Tables: {', '.join(tables)}\n\n"
                final_content += "\n\n".join(preview_parts)
                
                # Log d√©taill√©
                self.logger.info(
                    "üìã Aper√ßu de toutes les tables assembl√© via /records",
                    request_id=request_id,
                    document_id=document_id,
                    tables_count=len(tables),
                    tables_list=tables,
                    preview_size=len(final_content)
                )
                
                return final_content
            else:
                return "# Aucun contenu disponible"
                
        except Exception as e:
            self.logger.error(f"Erreur lors de la g√©n√©ration de l'aper√ßu: {str(e)}")
            return f"# Erreur lors de la g√©n√©ration de l'aper√ßu: {str(e)}"
    
    async def format_csv_content_for_analysis(self, document_id: str, table_id: str, request_id: str = "unknown", limit: int = 10) -> str:
        """R√©cup√®re et formate le contenu d'une table pour l'analyse via l'endpoint /records avec limite"""
        url = f"{self.base_url}/docs/{document_id}/tables/{table_id}/records"
        
        # Param√®tres pour limiter le nombre d'enregistrements
        params = {
            "limit": limit
        }
        
        try:
            async with httpx.AsyncClient(timeout=30.0) as client:
                response = await client.get(url, headers=self.headers, params=params)
                self.logger.log_grist_api(url, response.status_code)
                
                if response.status_code == 200:
                    data = response.json()
                    records = data.get("records", [])
                    
                    if not records:
                        return f"La table '{table_id}' ne contient aucun enregistrement."
                    
                    # Conversion des enregistrements en format CSV-like
                    csv_lines = []
                    
                    # R√©cup√©ration des noms de colonnes depuis le premier enregistrement
                    if records:
                        columns = list(records[0].get("fields", {}).keys())
                        csv_lines.append(",".join(columns))
                        
                        # Ajout des donn√©es
                        for record in records:
                            fields = record.get("fields", {})
                            row_values = []
                            for col in columns:
                                value = fields.get(col, "")
                                # Conversion en string et √©chappement des virgules
                                str_value = str(value) if value is not None else ""
                                # Limiter la taille de chaque cellule pour √©viter les contenus massifs
                                if len(str_value) > 200:  # Limite √† 200 caract√®res par cellule
                                    str_value = str_value[:200] + "..."
                                if "," in str_value:
                                    str_value = f'"{str_value}"'
                                row_values.append(str_value)
                            csv_lines.append(",".join(row_values))
                    
                    csv_content = "\n".join(csv_lines)
                    
                    # Formatage pour l'analyse
                    total_lines = len(csv_lines)
                    data_lines = total_lines - 1 if total_lines > 0 else 0
                    
                    formatted = f"Contenu de la table '{table_id}' ({data_lines} lignes de donn√©es r√©cup√©r√©es via /records):\n\n"
                    formatted += csv_content
                    
                    # Logs d√©taill√©s
                    self.logger.info(
                        "üìä Contenu pour analyse r√©cup√©r√© via /records",
                        request_id=request_id,
                        table_id=table_id,
                        records_count=len(records),
                        limit=limit,
                        content_size=len(formatted)
                    )
                    
                    return formatted
                    
                else:
                    error_msg = f"Erreur lors de la r√©cup√©ration des enregistrements de {table_id}: HTTP {response.status_code}"
                    self.logger.error(
                        f"Erreur lors de la r√©cup√©ration des enregistrements: {response.status_code}",
                        request_id=request_id,
                        table_id=table_id,
                        response_text=response.text
                    )
                    return f"Erreur lors de la r√©cup√©ration du contenu de la table {table_id}: {error_msg}"
                    
        except Exception as e:
            error_msg = f"Erreur lors de la r√©cup√©ration des enregistrements de {table_id}: {str(e)}"
            self.logger.error(f"Erreur lors de la r√©cup√©ration des enregistrements: {str(e)}", request_id=request_id)
            return error_msg
    
    def extract_csv_summary(self, csv_content: str, table_id: str = "Unknown") -> Dict[str, Any]:
        """Extrait un r√©sum√© simple du contenu CSV d'une table"""
        if csv_content.startswith("#"):
            return {"summary": "Erreur lors de la r√©cup√©ration", "table_id": table_id, "error": csv_content}
        
        try:
            lines = csv_content.strip().split('\n')
            line_count = len(lines)
            
            summary = {
                "table_id": table_id,
                "total_lines": line_count,
                "content_size": len(csv_content),
                "first_line": lines[0] if lines else "",
                "sample_lines": lines[:3] if len(lines) > 3 else lines
            }
            
            return summary
            
        except Exception as e:
            return {"summary": f"Erreur lors de l'analyse: {str(e)}", "table_id": table_id} 
            
 