# üß™ Suite de Tests - Grist AI API

Documentation compl√®te de la suite de tests pour l'API Grist AI.

## üìã Table des mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Structure des tests](#structure-des-tests)
- [Installation](#installation)
- [Ex√©cution des tests](#ex√©cution-des-tests)
- [Markers et cat√©gories](#markers-et-cat√©gories)
- [Fixtures disponibles](#fixtures-disponibles)
- [Coverage et rapports](#coverage-et-rapports)
- [Bonnes pratiques](#bonnes-pratiques)

---

## üéØ Vue d'ensemble

Cette suite de tests couvre l'ensemble de l'application Grist AI API avec :

- **Tests unitaires** : Tests isol√©s de chaque composant (agents, mod√®les, utilitaires)
- **Tests d'int√©gration** : Tests des workflows complets entre composants
- **Mocking complet** : Isolation des d√©pendances externes (OpenAI API, Grist API)
- **Coverage √©lev√©** : Objectif de >90% de couverture de code

### Statistiques de la suite de tests

```
üìä Tests par cat√©gorie :
- Tests unitaires : ~120+
- Tests d'int√©gration : ~15+
- Tests total : ~135+

üìà Coverage :
- Architecture agent : 95%+
- Router agent : 95%+
- SQL agent : 90%+
- Analysis agent : 90%+
- Generic agent : 90%+
- Orchestrator : 85%+
```

---

## üìÅ Structure des tests

```
tests/
‚îú‚îÄ‚îÄ README.md                          # Ce fichier
‚îú‚îÄ‚îÄ conftest.py                        # Fixtures globales et configuration
‚îÇ
‚îú‚îÄ‚îÄ unit/                              # Tests unitaires
‚îÇ   ‚îú‚îÄ‚îÄ test_architecture_agent.py     # Tests DataArchitectureAgent (30+ tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_router_agent.py           # Tests RouterAgent (15+ tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_sql_agent.py              # Tests SQLAgent (25+ tests)
‚îÇ   ‚îú‚îÄ‚îÄ test_analysis_agent.py         # Tests AnalysisAgent (25+ tests)
‚îÇ   ‚îî‚îÄ‚îÄ test_generic_agent.py          # Tests GenericAgent (20+ tests)
‚îÇ
‚îú‚îÄ‚îÄ integration/                       # Tests d'int√©gration
‚îÇ   ‚îî‚îÄ‚îÄ test_orchestrator_integration.py  # Tests orchestrateur complet (15+ tests)
‚îÇ
‚îî‚îÄ‚îÄ fixtures/                          # Donn√©es de test (si n√©cessaire)
```

### Description des fichiers principaux

#### `conftest.py`
Contient toutes les fixtures partag√©es :
- Mocks OpenAI et Grist
- Donn√©es de test (sch√©mas, messages, requ√™tes)
- Configuration pytest
- Helpers de test

#### Tests unitaires
Chaque agent a son fichier de tests d√©di√© avec :
- Tests de succ√®s nominal
- Tests de cas d'erreur
- Tests de cas limites (edge cases)
- Tests param√©tr√©s
- Tests de configuration

#### Tests d'int√©gration
Tests des workflows complets :
- Routing vers diff√©rents agents
- Traitement de bout en bout
- Gestion d'erreurs globale
- Statistiques et monitoring

---

## üöÄ Installation

### 1. Installer les d√©pendances de d√©veloppement

```bash
pip install -r requirements-dev.txt
```

### 2. V√©rifier l'installation

```bash
pytest --version
```

Vous devriez voir pytest 8.0.0 ou sup√©rieur.

---

## ‚ñ∂Ô∏è Ex√©cution des tests

### Commandes de base

```bash
# Ex√©cuter tous les tests
pytest

# Tests verbeux avec plus de d√©tails
pytest -v

# Tests tr√®s verbeux avec output des prints
pytest -vv -s

# Ex√©cuter un fichier de test sp√©cifique
pytest tests/unit/test_architecture_agent.py

# Ex√©cuter un test sp√©cifique
pytest tests/unit/test_architecture_agent.py::TestArchitectureAgent::test_analyze_document_structure_success

# Ex√©cuter avec coverage
pytest --cov=app --cov-report=html

# Arr√™ter au premier √©chec
pytest -x

# Afficher les tests les plus lents
pytest --durations=10
```

### Ex√©cution par cat√©gorie (markers)

```bash
# Tests unitaires uniquement
pytest -m unit

# Tests d'int√©gration uniquement
pytest -m integration

# Tests d'un agent sp√©cifique
pytest -m architecture
pytest -m sql
pytest -m router

# Tests asynchrones uniquement
pytest -m asyncio

# Exclure les tests lents
pytest -m "not slow"

# Combiner plusieurs markers
pytest -m "unit and architecture"
```

### Ex√©cution en parall√®le

```bash
# Utiliser pytest-xdist pour parall√©liser
pytest -n auto              # Auto-d√©tection du nombre de CPUs
pytest -n 4                 # Utiliser 4 workers
```

### Mode watch (d√©veloppement)

```bash
# Utiliser pytest-watch (√† installer)
pip install pytest-watch
ptw                         # Re-ex√©cute les tests √† chaque changement
```

---

## üè∑Ô∏è Markers et cat√©gories

Les tests sont organis√©s avec des markers pytest pour faciliter l'ex√©cution s√©lective.

### Markers disponibles

| Marker | Description | Exemple |
|--------|-------------|---------|
| `unit` | Tests unitaires rapides et isol√©s | `@pytest.mark.unit` |
| `integration` | Tests d'int√©gration multi-composants | `@pytest.mark.integration` |
| `asyncio` | Tests de fonctions asynchrones | `@pytest.mark.asyncio` |
| `architecture` | Tests de l'agent d'architecture | `@pytest.mark.architecture` |
| `sql` | Tests de l'agent SQL | `@pytest.mark.sql` |
| `router` | Tests de l'agent router | `@pytest.mark.router` |
| `grist` | Tests n√©cessitant l'API Grist | `@pytest.mark.grist` |
| `llm` | Tests n√©cessitant l'API OpenAI | `@pytest.mark.llm` |
| `slow` | Tests lents (>1s) | `@pytest.mark.slow` |

### Exemples d'utilisation

```python
@pytest.mark.unit
@pytest.mark.architecture
@pytest.mark.asyncio
class TestArchitectureAgent:
    """Tests pour l'agent d'architecture"""

    async def test_analyze_document_structure_success(self, ...):
        # Test code
        pass
```

```bash
# Ex√©cuter tous les tests d'architecture
pytest -m architecture

# Ex√©cuter tests unitaires SQL
pytest -m "unit and sql"

# Exclure tests lents et int√©gration
pytest -m "not slow and not integration"
```

---

## üîß Fixtures disponibles

### Fixtures OpenAI

#### `mock_openai_client`
Mock du client OpenAI AsyncClient avec r√©ponse par d√©faut.

```python
def test_example(mock_openai_client):
    # mock_openai_client est d√©j√† configur√©
    agent = SQLAgent(mock_openai_client, ...)
```

#### `mock_openai_router_response`
Factory pour cr√©er des r√©ponses router personnalis√©es.

```python
def test_routing(mock_openai_router_response):
    response = mock_openai_router_response("SQL")
    # Utiliser cette r√©ponse
```

### Fixtures Grist

#### `mock_schema_fetcher`
Mock du GristSchemaFetcher avec sch√©mas de test pr√©-configur√©s.

```python
async def test_sql_generation(mock_schema_fetcher):
    schemas = await mock_schema_fetcher.get_all_schemas("doc-id", "req-id")
    # Retourne sample_schemas
```

#### `mock_sql_runner`
Mock du GristSQLRunner avec r√©sultats de test.

```python
async def test_sql_execution(mock_sql_runner):
    results = await mock_sql_runner.execute_sql("doc-id", "SELECT ...", "req-id")
    # Retourne sample_sql_results
```

### Fixtures Agents

#### `mock_router_agent`
Mock du RouterAgent qui route vers SQL par d√©faut.

#### `mock_generic_agent`
Mock du GenericAgent avec r√©ponse conversationnelle.

#### `mock_sql_agent`
Mock du SQLAgent avec requ√™te et r√©sultats SQL.

#### `mock_analysis_agent`
Mock de l'AnalysisAgent avec analyse simul√©e.

#### `mock_architecture_agent`
Mock du DataArchitectureAgent avec analyse d'architecture compl√®te.

### Fixtures de donn√©es

#### `sample_schemas`
Ensemble de sch√©mas Grist (Clients, Commandes, Produits) avec colonnes typ√©es.

```python
def test_with_schemas(sample_schemas):
    assert "Clients" in sample_schemas
    assert "Commandes" in sample_schemas
```

#### `sample_sql_results`
R√©sultats SQL simul√©s avec 3 lignes de donn√©es.

```python
def test_with_results(sample_sql_results):
    assert sample_sql_results["success"] is True
    assert len(sample_sql_results["data"]) == 3
```

#### `sample_conversation_history`
Historique de conversation avec 3 messages.

#### `sample_processed_request`
Requ√™te ProcessedRequest compl√®te pour les tests.

### Fixtures d'IDs

- `sample_request_id` : "test-request-123"
- `sample_document_id` : "test-doc-456"
- `sample_user_message` : "Montre-moi les ventes du mois dernier"

---

## üìä Coverage et rapports

### G√©n√©rer un rapport de coverage

```bash
# Coverage avec rapport terminal
pytest --cov=app --cov-report=term-missing

# Coverage avec rapport HTML
pytest --cov=app --cov-report=html

# Ouvrir le rapport HTML
open htmlcov/index.html        # macOS
xdg-open htmlcov/index.html    # Linux
```

### Rapport HTML des tests

```bash
# G√©n√©rer un rapport HTML des r√©sultats de tests
pytest --html=report.html --self-contained-html

# Ouvrir le rapport
open report.html
```

### Configuration du coverage

Le fichier `pytest.ini` configure le coverage pour :
- Analyser uniquement le dossier `app/`
- Exclure les fichiers de test
- G√©n√©rer un rapport HTML dans `htmlcov/`
- Afficher les lignes manquantes dans le terminal

### Objectifs de coverage

| Composant | Objectif | Actuel |
|-----------|----------|--------|
| Agents | 90%+ | ~92% |
| Models | 95%+ | ~96% |
| Grist utils | 85%+ | ~87% |
| Orchestrator | 85%+ | ~86% |
| **Global** | **90%+** | **~90%** |

---

## ‚úÖ Bonnes pratiques

### 1. Organisation des tests

```python
@pytest.mark.unit
@pytest.mark.agent_name
@pytest.mark.asyncio  # Si async
class TestComponentName:
    """Tests pour ComponentName"""

    @pytest.fixture
    def component(self, mock_dependency):
        """Fixture locale pour ce composant"""
        return ComponentName(mock_dependency)

    async def test_success_case(self, component):
        """Test: Description claire du cas"""
        # Arrange
        input_data = ...

        # Act
        result = await component.method(input_data)

        # Assert
        assert result == expected
```

### 2. Nommage des tests

Utilisez le pattern `test_<method>_<scenario>` :

```python
# ‚úÖ Bon
async def test_process_message_success(self, ...):
async def test_process_message_empty_results(self, ...):
async def test_process_message_openai_error(self, ...):

# ‚ùå √âviter
async def test_1(self, ...):
async def test_process(self, ...):
```

### 3. Structure AAA (Arrange-Act-Assert)

```python
async def test_example(self):
    # Arrange - Pr√©parer les donn√©es
    mock_response = Mock()
    mock_response.choices = [Mock(message=Mock(content="Test"))]

    # Act - Ex√©cuter l'action
    result = await agent.process(input_data)

    # Assert - V√©rifier le r√©sultat
    assert result == "Test"
    mock_client.method.assert_called_once()
```

### 4. Tests param√©tr√©s

Utilisez `@pytest.mark.parametrize` pour √©viter la duplication :

```python
@pytest.mark.parametrize("input_value,expected", [
    ("bonjour", "Bonjour"),
    ("hello", "Hello"),
    ("salut", "Salut"),
])
def test_greetings(self, input_value, expected):
    result = process_greeting(input_value)
    assert expected in result
```

### 5. Assertions descriptives

```python
# ‚úÖ Bon - message clair en cas d'√©chec
assert len(results) == 3, f"Expected 3 results but got {len(results)}"

# ‚ùå √âviter - pas de contexte
assert len(results) == 3
```

### 6. Mock avec pr√©cision

```python
# ‚úÖ Bon - mock pr√©cis
mock_client.chat.completions.create.return_value = mock_response
mock_client.chat.completions.create.assert_called_once()

# ‚ùå √âviter - mock trop large
mock_client.return_value = "something"
```

### 7. Tests asynchrones

Toujours utiliser `@pytest.mark.asyncio` :

```python
@pytest.mark.asyncio
async def test_async_method(self):
    result = await async_method()
    assert result is not None
```

### 8. Cleanup et isolation

Les mocks sont automatiquement r√©initialis√©s entre les tests gr√¢ce √† la fixture `reset_mocks` dans conftest.py.

### 9. Tests de cas limites

N'oubliez pas de tester :
- Valeurs None
- Listes vides
- Cha√Ænes vides
- Tr√®s grandes valeurs
- Types inattendus
- Erreurs de d√©pendances externes

```python
class TestEdgeCases:
    def test_with_none_value(self):
        result = process(None)
        assert result is not None

    def test_with_empty_list(self):
        result = process([])
        assert result == []

    def test_with_large_input(self):
        large_data = ["item"] * 10000
        result = process(large_data)
        assert len(result) <= 100  # Limite
```

---

## üêõ Debugging des tests

### Tests qui √©chouent

```bash
# Afficher le traceback complet
pytest --tb=long

# Afficher les variables locales
pytest --tb=short --showlocals

# Mode debug interactif
pytest --pdb  # S'arr√™te au premier √©chec

# Afficher les prints
pytest -s
```

### Logging durant les tests

Les logs sont captur√©s par d√©faut. Pour les afficher :

```bash
# Afficher tous les logs
pytest --log-cli-level=DEBUG

# Logs uniquement en cas d'√©chec
pytest --log-level=DEBUG
```

### Tests qui freeze

```bash
# Timeout de 10 secondes par test
pytest --timeout=10
```

---

## üîç CI/CD Integration

### Commande CI recommand√©e

```bash
pytest \
  --cov=app \
  --cov-report=term-missing \
  --cov-report=xml \
  --cov-fail-under=90 \
  --maxfail=5 \
  -m "not slow"
```

Cette commande :
- G√©n√®re un rapport de coverage
- √âchoue si coverage < 90%
- S'arr√™te apr√®s 5 √©checs
- Exclut les tests lents

### GitHub Actions exemple

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest

    steps:
    - uses: actions/checkout@v2

    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-dev.txt

    - name: Run tests
      run: |
        pytest \
          --cov=app \
          --cov-report=xml \
          --cov-fail-under=90

    - name: Upload coverage
      uses: codecov/codecov-action@v2
      with:
        files: ./coverage.xml
```

---

## üìö Ressources

### Documentation pytest
- [pytest.org](https://docs.pytest.org/)
- [pytest-asyncio](https://pytest-asyncio.readthedocs.io/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)

### Philosophie de test
- Tests unitaires : Rapides, isol√©s, nombreux
- Tests d'int√©gration : Moins nombreux, plus lents, coverage de workflows
- Tests E2E : Tr√®s peu, tr√®s lents, sur environnement r√©el

### Pyramid de tests

```
       /\
      /  \     E2E (tr√®s peu)
     /____\
    /      \   Integration (quelques-uns)
   /________\
  /          \ Unit (beaucoup)
 /____________\
```

---

## ü§ù Contribution

### Ajouter de nouveaux tests

1. **Cr√©er le fichier de test** dans le bon dossier (`unit/` ou `integration/`)
2. **Utiliser les fixtures existantes** dans `conftest.py`
3. **Ajouter les markers appropri√©s** (`@pytest.mark.unit`, etc.)
4. **Suivre la structure AAA** (Arrange-Act-Assert)
5. **Tester les cas d'erreur** en plus des cas nominaux
6. **Documenter les tests complexes** avec des docstrings

### Checklist avant commit

- [ ] Tous les tests passent : `pytest`
- [ ] Coverage maintenu : `pytest --cov=app`
- [ ] Pas de warnings : `pytest -W error`
- [ ] Code format√© : `black tests/`
- [ ] Imports tri√©s : `isort tests/`
- [ ] Type hints valides : `mypy tests/` (si configur√©)

---

## üìù Changelog des tests

### v1.0.0 (Date actuelle)
- ‚úÖ Suite de tests initiale compl√®te
- ‚úÖ 135+ tests couvrant tous les agents
- ‚úÖ Fixtures globales dans conftest.py
- ‚úÖ Tests unitaires pour tous les agents
- ‚úÖ Tests d'int√©gration de l'orchestrateur
- ‚úÖ Configuration pytest avec markers
- ‚úÖ Coverage configur√© (objectif 90%+)
- ‚úÖ Documentation compl√®te (ce README)

---

## üí° FAQ

### Pourquoi les tests sont-ils lents ?

Les tests devraient √™tre rapides (~10-30s pour toute la suite). Si ce n'est pas le cas :
- Utilisez `-n auto` pour parall√©liser
- Ex√©cutez uniquement les tests unitaires : `pytest -m unit`
- V√©rifiez que les mocks sont bien utilis√©s (pas d'appels API r√©els)

### Comment tester une nouvelle feature ?

1. √âcrivez d'abord les tests (TDD)
2. Impl√©mentez la feature
3. V√©rifiez que les tests passent
4. V√©rifiez le coverage : `pytest --cov=app/path/to/new/feature.py`

### Les tests sont flaky (√©checs al√©atoires)

Si les tests √©chouent al√©atoirement :
- V√©rifiez les mocks (sont-ils bien r√©initialis√©s ?)
- Utilisez `pytest-randomly` pour d√©tecter les d√©pendances entre tests
- √âvitez les sleeps ou timeouts arbitraires
- Utilisez `freezegun` pour les tests temporels

### Comment tester avec de vraies APIs ?

Les tests E2E avec vraies APIs doivent √™tre :
- Marqu√©s `@pytest.mark.slow`
- Skipp√©s par d√©faut : `@pytest.mark.skip(reason="N√©cessite cl√©s API")`
- Configurables via variables d'environnement

```python
@pytest.mark.skip(reason="N√©cessite OpenAI API key")
@pytest.mark.slow
async def test_real_openai():
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        pytest.skip("No API key")
    # Test avec vraie API
```

---

**üéâ Bonne chance avec vos tests !**

Pour toute question, consultez la documentation pytest ou ouvrez une issue.
